{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fa43b69",
   "metadata": {},
   "source": [
    "# Region-Based Image Segmentation Techniques\n",
    "\n",
    "**DCIT407 - Image Processing Semester Project**  \n",
    "**Group 19**\n",
    "\n",
    "---\n",
    "\n",
    "## Project Information\n",
    "\n",
    "**Topic:** Techniques used in region-based image segmentation\n",
    "\n",
    "**Team Members:**\n",
    "\n",
    "| Name | Student ID | Role | Technique(s) |\n",
    "|------|-----------|------|-------------|\n",
    "| Ryan Nii Akwei Brown | 11357610 | Group Leader | CNN + Transformer Hybrid (Deep Learning) |\n",
    "| Cyril Ashong | 11253767 | Member | Watershed Segmentation |\n",
    "| Prince Henry Kissi | 11063475 | Member | Diffusion-Based Segmentation |\n",
    "| Martey Kelvin Mamah | 11237476 | Member | Mean Shift Segmentation |\n",
    "| Ebenezer Nii Nai Badger | 11290659 | Member | Active Contours/Snakes |\n",
    "| Harriet Esinam Kale | 11357530 | Member | Split and Merge |\n",
    "| Delina Awash Welday | 11358725 | Member | Region Growing & Self-Supervised Segmentation |\n",
    "| Owusu Emmanuel Takyi | 11264083 | Member | Graph-Based Segmentation (Felzenszwalb) |\n",
    "\n",
    "**Project Start Date:** February 6, 2026  \n",
    "**Submission Date:** February 17, 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Theory: Region-Based Image Segmentation](#theory)\n",
    "3. [Mathematical Foundations](#mathematics)\n",
    "4. [Individual Technique Implementations](#implementations)\n",
    "   - 4.1 [Region Growing (Delina)](#region-growing)\n",
    "   - 4.2 [Watershed Segmentation (Cyril)](#watershed)\n",
    "   - 4.3 [Mean Shift Segmentation (Kelvin)](#mean-shift)\n",
    "   - 4.4 [Split and Merge (Harriet)](#split-merge)\n",
    "   - 4.5 [Active Contours/Snakes (Eben)](#active-contours)\n",
    "   - 4.6 [Graph-Based Segmentation (Takyi)](#graph-based)\n",
    "   - 4.7 [CNN + Transformer Hybrid (Ryan)](#cnn-transformer)\n",
    "   - 4.8 [Self-Supervised Segmentation (Delina)](#self-supervised)\n",
    "   - 4.9 [Diffusion-Based Segmentation (Henry)](#diffusion)\n",
    "5. [Comparative Analysis](#comparison)\n",
    "6. [Conclusion](#conclusion)\n",
    "7. [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "## 1. Introduction\n",
    "\n",
    "### What is Image Segmentation?\n",
    "\n",
    "Image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels, also known as regions or objects). The goal is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze. In essence, segmentation assigns a label to every pixel in an image such that pixels with the same label share certain characteristics.\n",
    "\n",
    "### Region-Based Segmentation\n",
    "\n",
    "Region-based segmentation methods analyze images by grouping pixels into regions based on predefined criteria. Unlike edge-based methods that focus on boundaries, region-based approaches focus on the interior properties of regions such as:\n",
    "\n",
    "- **Intensity/Color similarity**: Pixels with similar intensity or color values\n",
    "- **Texture**: Patterns and spatial arrangements of intensities\n",
    "- **Spatial coherence**: Grouping neighboring pixels\n",
    "- **Statistical properties**: Mean, variance, and other statistical measures\n",
    "\n",
    "### Project Scope\n",
    "\n",
    "This project explores **9 different segmentation techniques** spanning classical algorithms to modern deep learning approaches:\n",
    "\n",
    "**Classical Methods (1980s-2000s):**\n",
    "1. Region Growing\n",
    "2. Watershed Segmentation\n",
    "3. Mean Shift Segmentation\n",
    "4. Split and Merge\n",
    "5. Active Contours (Snakes)\n",
    "6. Graph-Based Segmentation\n",
    "\n",
    "**Modern Deep Learning Methods (2020s):**\n",
    "7. CNN + Transformer Hybrid (SegFormer)\n",
    "8. Self-Supervised Segmentation\n",
    "9. Diffusion-Based Segmentation\n",
    "\n",
    "Each technique is implemented in a separate Jupyter notebook by different team members, following the project guidelines outlined in the DCIT407 semester project directives.\n",
    "\n",
    "### Project Structure\n",
    "\n",
    "```\n",
    "Group19_RegionSegmentation/\n",
    "├── README.md                                              # Project overview\n",
    "├── REGION_SEGMENTATION_PROJECT.ipynb                      # This master notebook\n",
    "├── Delina_RegionGrowing_SelfSupervised.ipynb             # Individual implementations\n",
    "├── Cyril_Ashong_Watershed_Segmentation.ipynb\n",
    "├── Kelvin_MeanShift_Segmentation.ipynb\n",
    "├── Harriet_split_merge_segmentation.ipynb\n",
    "├── Eben_ActiveContours.ipynb\n",
    "├── Takyi_GraphBasedSegmentation.ipynb\n",
    "├── Ryan_CNN_Transformer_Hybrid.ipynb\n",
    "├── HenryKissi_DiffusionBasedSegmentation.ipynb\n",
    "├── images/                                                # Sample images\n",
    "└── results/                                               # Output visualizations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theory",
   "metadata": {},
   "source": [
    "<a id=\"theory\"></a>\n",
    "## 2. Theory: Region-Based Image Segmentation\n",
    "\n",
    "### Fundamental Concepts\n",
    "\n",
    "#### 1. **Region Growing**\n",
    "Starts from seed points and grows regions by adding neighboring pixels that satisfy similarity criteria (e.g., intensity difference threshold). The process continues until no more pixels can be added.\n",
    "\n",
    "#### 2. **Region Splitting and Merging**\n",
    "Uses a divide-and-conquer approach. Initially splits the image into quadrants recursively until each region satisfies a homogeneity criterion, then merges adjacent regions that are similar.\n",
    "\n",
    "#### 3. **Watershed Segmentation**\n",
    "Treats the gradient magnitude of an image as a topographic surface. Regions are formed by \"flooding\" from local minima, with watersheds (boundaries) occurring where different floods meet.\n",
    "\n",
    "#### 4. **Mean Shift Segmentation**\n",
    "An iterative mode-seeking algorithm that shifts each pixel to the mean of pixels within a defined kernel, effectively clustering pixels in the feature space (color + spatial position).\n",
    "\n",
    "#### 5. **Active Contours (Snakes)**\n",
    "Deformable curves that evolve to minimize an energy functional, balancing internal forces (smoothness) and external forces (image features like edges).\n",
    "\n",
    "#### 6. **Graph-Based Segmentation**\n",
    "Represents the image as a graph where pixels are nodes and edges connect neighboring pixels with weights based on similarity. Segmentation is achieved by partitioning the graph.\n",
    "\n",
    "#### 7. **CNN + Transformer Hybrids**\n",
    "Modern deep learning approaches that combine Convolutional Neural Networks (for local feature extraction) with Transformers (for global context modeling) to achieve semantic segmentation.\n",
    "\n",
    "#### 8. **Self-Supervised Segmentation**\n",
    "Learns visual representations from unlabeled data using pretext tasks, then applies these learned features to segmentation without requiring extensive labeled datasets.\n",
    "\n",
    "#### 9. **Diffusion-Based Segmentation**\n",
    "Leverages diffusion models (denoising processes) to iteratively refine segmentation masks, representing the cutting edge of research in 2023-2026.\n",
    "\n",
    "### Key Differences: Classical vs. Deep Learning\n",
    "\n",
    "| Aspect | Classical Methods | Deep Learning Methods |\n",
    "|--------|------------------|----------------------|\n",
    "| **Approach** | Hand-crafted rules and algorithms | Learned from data |\n",
    "| **Training** | No training required | Requires large labeled datasets |\n",
    "| **Semantic Understanding** | No - groups similar pixels | Yes - understands object classes |\n",
    "| **Computational Cost** | Low to Medium | High (GPU recommended) |\n",
    "| **Adaptability** | Fixed rules, less adaptable | Adapts to training data |\n",
    "| **Interpretability** | High - explicit rules | Low - \"black box\" |\n",
    "| **Performance** | Good for simple scenes | Excellent for complex scenes |\n",
    "\n",
    "### Evolution of Segmentation\n",
    "\n",
    "```\n",
    "1970s-1980s: Basic region-based methods (region growing, split-merge)\n",
    "1990s: Watershed, graph-based, active contours\n",
    "2000s: Mean shift, improved graph algorithms\n",
    "2010s: Deep learning revolution (CNNs for segmentation)\n",
    "2020s: Transformer-based models, self-supervised learning, diffusion models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematics",
   "metadata": {},
   "source": [
    "<a id=\"mathematics\"></a>\n",
    "## 3. Mathematical Foundations\n",
    "\n",
    "### 3.1 Region Growing\n",
    "\n",
    "**Basic Algorithm:**\n",
    "1. Select seed point(s) $S = \\{s_1, s_2, ..., s_n\\}$\n",
    "2. For each pixel $p$ adjacent to region $R$:\n",
    "   $$\\text{Add } p \\text{ to } R \\text{ if } |I(p) - \\mu_R| < T$$\n",
    "   where $I(p)$ is intensity at $p$, $\\mu_R$ is mean intensity of region $R$, and $T$ is threshold\n",
    "\n",
    "**Homogeneity Criterion:**\n",
    "$$H(R) = \\begin{cases} \n",
    "\\text{true} & \\text{if } \\sigma_R^2 < T_{\\sigma} \\\\\n",
    "\\text{false} & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Watershed Transform\n",
    "\n",
    "**Gradient Magnitude:**\n",
    "$$G = \\sqrt{\\left(\\frac{\\partial I}{\\partial x}\\right)^2 + \\left(\\frac{\\partial I}{\\partial y}\\right)^2}$$\n",
    "\n",
    "**Flooding Process:**\n",
    "- Treat $G$ as topographic surface\n",
    "- Flood from local minima\n",
    "- Watershed lines form where floods meet\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Mean Shift Segmentation\n",
    "\n",
    "**Mean Shift Vector:**\n",
    "$$M(x) = \\frac{\\sum_{x_i \\in N(x)} K(x - x_i) \\cdot x_i}{\\sum_{x_i \\in N(x)} K(x - x_i)} - x$$\n",
    "\n",
    "where $K$ is a kernel function (typically Gaussian) and $N(x)$ is neighborhood of $x$.\n",
    "\n",
    "**Iterative Update:**\n",
    "$$x_{t+1} = x_t + M(x_t)$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4 Split and Merge\n",
    "\n",
    "**Split Criterion (Quadtree):**\n",
    "$$\\text{Split } R \\text{ if } \\sigma_R^2 > T \\text{ or } \\max(I) - \\min(I) > T$$\n",
    "\n",
    "**Merge Criterion:**\n",
    "$$\\text{Merge } R_i, R_j \\text{ if } |\\mu_i - \\mu_j| < T_m$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3.5 Active Contours (Snakes)\n",
    "\n",
    "**Energy Functional:**\n",
    "$$E_{snake} = E_{internal} + E_{external}$$\n",
    "\n",
    "**Internal Energy (Smoothness):**\n",
    "$$E_{internal} = \\int_0^1 \\left[\\alpha|v'(s)|^2 + \\beta|v''(s)|^2\\right] ds$$\n",
    "\n",
    "**External Energy (Image Features):**\n",
    "$$E_{external} = -\\int_0^1 |\\nabla I(v(s))|^2 ds$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3.6 Graph-Based Segmentation\n",
    "\n",
    "**Graph Representation:**\n",
    "- Nodes: Pixels\n",
    "- Edges: Connections between neighboring pixels\n",
    "- Weights: $w(p_i, p_j) = |I(p_i) - I(p_j)|$\n",
    "\n",
    "**Minimum Spanning Tree (MST) Clustering:**\n",
    "- Build MST of graph\n",
    "- Remove edges with weight > threshold\n",
    "- Connected components = segments\n",
    "\n",
    "---\n",
    "\n",
    "### 3.7 CNN + Transformer Hybrid (SegFormer)\n",
    "\n",
    "**Self-Attention Mechanism:**\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "**Multi-Scale Feature Extraction:**\n",
    "- Hierarchical encoder: Features at $\\frac{1}{4}, \\frac{1}{8}, \\frac{1}{16}, \\frac{1}{32}$ resolution\n",
    "- MLP decoder: Fuses multi-scale features → per-pixel predictions\n",
    "\n",
    "---\n",
    "\n",
    "### 3.8 Similarity Measures\n",
    "\n",
    "**Euclidean Distance (Intensity):**\n",
    "$$d(p_1, p_2) = |I(p_1) - I(p_2)|$$\n",
    "\n",
    "**Color Distance (RGB):**\n",
    "$$d_{color}(p_1, p_2) = \\sqrt{(R_1-R_2)^2 + (G_1-G_2)^2 + (B_1-B_2)^2}$$\n",
    "\n",
    "**Variance (Region Homogeneity):**\n",
    "$$\\sigma_R^2 = \\frac{1}{|R|} \\sum_{p \\in R} (I(p) - \\mu_R)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "implementations",
   "metadata": {},
   "source": [
    "<a id=\"implementations\"></a>\n",
    "## 4. Individual Technique Implementations\n",
    "\n",
    "Each team member has implemented one or more segmentation techniques in separate Jupyter notebooks. Below are the details and links to each implementation.\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"region-growing\"></a>\n",
    "### 4.1 Region Growing (Delina Awash Welday)\n",
    "\n",
    "**Notebook:** `Delina_Region Growing_ Self-Supervised Segmentation.ipynb`\n",
    "\n",
    "**Technique Overview:**\n",
    "- Seed-based pixel expansion using intensity threshold\n",
    "- Grows regions by iteratively adding similar neighboring pixels\n",
    "- Effective for images with well-defined regions of uniform intensity\n",
    "\n",
    "**Implementation Details:**\n",
    "- Uses custom Python implementation with NumPy\n",
    "- Dataset: Grayscale images (e.g., Lena, medical scans)\n",
    "- Key parameters: seed points, similarity threshold\n",
    "\n",
    "**To view this implementation:**\n",
    "```python\n",
    "# Open the notebook:\n",
    "# Delina_Region Growing_ Self-Supervised Segmentation.ipynb\n",
    "```\n",
    "\n",
    "**Applications:**\n",
    "- Medical imaging (tumor detection)\n",
    "- Satellite imagery analysis\n",
    "- Agricultural crop monitoring\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"watershed\"></a>\n",
    "### 4.2 Watershed Segmentation (Cyril Ashong)\n",
    "\n",
    "**Notebook:** `Cyril_Ashong_Watershed_Segmentation.ipynb`\n",
    "\n",
    "**Technique Overview:**\n",
    "- Distance transform + markers → `cv2.watershed()`\n",
    "- Treats image gradient as topographic surface\n",
    "- Excellent for separating touching objects\n",
    "\n",
    "**Implementation Details:**\n",
    "- Uses OpenCV's watershed implementation\n",
    "- Dataset: Natural images (coins, cells)\n",
    "- Preprocessing: morphological operations for marker detection\n",
    "\n",
    "**To view this implementation:**\n",
    "```python\n",
    "# Open the notebook:\n",
    "# Cyril_Ashong_Watershed_Segmentation.ipynb\n",
    "```\n",
    "\n",
    "**Applications:**\n",
    "- Cell counting in microscopy\n",
    "- Coin separation\n",
    "- Document analysis\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"mean-shift\"></a>\n",
    "### 4.3 Mean Shift Segmentation (Martey Kelvin Mamah)\n",
    "\n",
    "**Notebook:** `Kelvin_MeanShift_Segmentation.ipynb` *(To be added)*\n",
    "\n",
    "**Technique Overview:**\n",
    "- `cv2.pyrMeanShiftFiltering()` for clustering-based segmentation\n",
    "- Mode-seeking algorithm in color-spatial space\n",
    "- Automatically determines number of segments\n",
    "\n",
    "**Implementation Details:**\n",
    "- Uses OpenCV and scikit-learn\n",
    "- Dataset: Color images\n",
    "- Key parameters: spatial radius, color radius\n",
    "\n",
    "**Applications:**\n",
    "- Video segmentation\n",
    "- Object tracking\n",
    "- Color-based image analysis\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"split-merge\"></a>\n",
    "### 4.4 Split and Merge (Harriet Esinam Kale)\n",
    "\n",
    "**Notebook:** `Harriet_split_merge_segmentation.ipynb`\n",
    "\n",
    "**Technique Overview:**\n",
    "- Recursive quadtree split until homogeneity achieved\n",
    "- Then merge adjacent similar regions\n",
    "- Divide-and-conquer approach\n",
    "\n",
    "**Implementation Details:**\n",
    "- Custom Python implementation with NumPy\n",
    "- Dataset: Synthetic images (checkerboard, shapes)\n",
    "- Homogeneity criterion: variance threshold\n",
    "\n",
    "**To view this implementation:**\n",
    "```python\n",
    "# Open the notebook:\n",
    "# Harriet_split_merge_segmentation.ipynb\n",
    "```\n",
    "\n",
    "**Applications:**\n",
    "- Texture analysis\n",
    "- Aerial image segmentation\n",
    "- Quadtree-based image compression\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"active-contours\"></a>\n",
    "### 4.5 Active Contours / Snakes (Ebenezer Nii Nai Badger)\n",
    "\n",
    "**Notebook:** `Eben_ActiveContours.ipynb`\n",
    "\n",
    "**Technique Overview:**\n",
    "- `skimage.segmentation.active_contour()` with initial contour\n",
    "- Deformable curve evolution to minimize energy\n",
    "- Balances smoothness and image features\n",
    "\n",
    "**Implementation Details:**\n",
    "- Uses scikit-image\n",
    "- Dataset: Medical or shape images\n",
    "- Requires manual initialization of contour\n",
    "\n",
    "**To view this implementation:**\n",
    "```python\n",
    "# Open the notebook:\n",
    "# Eben_ActiveContours.ipynb\n",
    "```\n",
    "\n",
    "**Applications:**\n",
    "- Medical image segmentation (organ boundaries)\n",
    "- Object tracking in video\n",
    "- Shape analysis\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"graph-based\"></a>\n",
    "### 4.6 Graph-Based Segmentation - Felzenszwalb (Owusu Emmanuel Takyi)\n",
    "\n",
    "**Notebook:** `Takyi_GraphBasedSegmentation(Felzenszwalb).ipynb`\n",
    "\n",
    "**Technique Overview:**\n",
    "- `skimage.segmentation.felzenszwalb()` for efficient graph clustering\n",
    "- Represents image as graph, segments via minimum spanning tree\n",
    "- Fast and parameter-efficient\n",
    "\n",
    "**Implementation Details:**\n",
    "- Uses scikit-image\n",
    "- Dataset: Natural images\n",
    "- Key parameters: scale, sigma, min_size\n",
    "\n",
    "**To view this implementation:**\n",
    "```python\n",
    "# Open the notebook:\n",
    "# Takyi_GraphBasedSegmentation(Felzenszwalb).ipynb\n",
    "```\n",
    "\n",
    "**Applications:**\n",
    "- Natural image segmentation\n",
    "- Scene parsing\n",
    "- Photo editing tools\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"cnn-transformer\"></a>\n",
    "### 4.7 CNN + Transformer Hybrid (Ryan Nii Akwei Brown)\n",
    "\n",
    "**Notebook:** `Ryan_CNN_Transformer_Hybrid.ipynb`\n",
    "\n",
    "**Technique Overview:**\n",
    "- SegFormer-B0: Hierarchical Transformer encoder + MLP decoder\n",
    "- Combines CNN local features with Transformer global context\n",
    "- State-of-the-art semantic segmentation\n",
    "\n",
    "**Implementation Details:**\n",
    "- Uses PyTorch, Hugging Face Transformers, segmentation_models_pytorch\n",
    "- Pretrained model: SegFormer-B0 (Cityscapes dataset)\n",
    "- 19 urban scene classes\n",
    "\n",
    "**To view this implementation:**\n",
    "```python\n",
    "# Open the notebook:\n",
    "# Ryan_CNN_Transformer_Hybrid.ipynb\n",
    "```\n",
    "\n",
    "**Applications:**\n",
    "- Autonomous driving (Tesla, Waymo)\n",
    "- Urban planning from satellite imagery\n",
    "- Augmented reality scene understanding\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"self-supervised\"></a>\n",
    "### 4.8 Self-Supervised Segmentation (Delina Awash Welday)\n",
    "\n",
    "**Notebook:** `Delina_Region Growing_ Self-Supervised Segmentation.ipynb`\n",
    "\n",
    "**Technique Overview:**\n",
    "- Demo pretrained SSL backbone (e.g., DINO) → cluster embeddings\n",
    "- Learns visual representations from unlabeled data\n",
    "- Reduces dependency on manual annotations\n",
    "\n",
    "**Implementation Details:**\n",
    "- Uses torchvision, lightly\n",
    "- Dataset: Small unlabeled dataset\n",
    "- Pretrained SSL model for feature extraction\n",
    "\n",
    "**Applications:**\n",
    "- Medical imaging (limited labeled data)\n",
    "- Domain adaptation\n",
    "- Low-resource segmentation scenarios\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"diffusion\"></a>\n",
    "### 4.9 Diffusion-Based Segmentation (Prince Henry Kissi)\n",
    "\n",
    "**Notebook:** `HenryKissi_DiffusionBasedSegmentation.ipynb`\n",
    "\n",
    "**Technique Overview:**\n",
    "- Shows how diffusion models refine segmentation masks\n",
    "- Cutting-edge research frontier (2023-2026)\n",
    "- Iterative denoising for high-quality results\n",
    "\n",
    "**Implementation Details:**\n",
    "- Uses diffusers library, PyTorch\n",
    "- Pretrained diffusion model\n",
    "- Demonstrates mask refinement process\n",
    "\n",
    "**To view this implementation:**\n",
    "```python\n",
    "# Open the notebook:\n",
    "# HenryKissi_DiffusionBasedSegmentation.ipynb\n",
    "```\n",
    "\n",
    "**Applications:**\n",
    "- High-quality medical segmentation\n",
    "- Few-shot learning scenarios\n",
    "- Research and experimental applications\n",
    "\n",
    "---\n",
    "\n",
    "## How to Access Individual Notebooks\n",
    "\n",
    "All individual implementation notebooks are stored in the same directory as this master notebook. To view a specific implementation:\n",
    "\n",
    "```python\n",
    "# In Jupyter:\n",
    "# 1. Go to File → Open\n",
    "# 2. Navigate to the project directory\n",
    "# 3. Click on the desired notebook\n",
    "\n",
    "# Or use direct links in JupyterLab/VSCode\n",
    "```\n",
    "\n",
    "Each notebook follows the standardized structure:\n",
    "1. Title & Project Info\n",
    "2. Table of Contents\n",
    "3. Introduction\n",
    "4. Theoretical Foundation\n",
    "5. Methodology\n",
    "6. Implementation (code)\n",
    "7. Results (visualizations)\n",
    "8. Discussion (strengths, limitations, applications)\n",
    "9. Conclusion\n",
    "10. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "<a id=\"comparison\"></a>\n",
    "## 5. Comparative Analysis\n",
    "\n",
    "### Performance Characteristics\n",
    "\n",
    "| Technique | Computational Cost | Parameter Tuning | Dataset Size Needed | Semantic Understanding |\n",
    "|-----------|-------------------|------------------|---------------------|------------------------|\n",
    "| **Region Growing** | Low | Medium | N/A (unsupervised) | No |\n",
    "| **Watershed** | Low-Medium | Low | N/A | No |\n",
    "| **Mean Shift** | Medium | Medium | N/A | No |\n",
    "| **Split & Merge** | Medium | Medium | N/A | No |\n",
    "| **Active Contours** | Medium | High | N/A | No |\n",
    "| **Graph-Based** | Medium | Low | N/A | No |\n",
    "| **CNN+Transformer** | High (GPU) | Low (pretrained) | Large (for training) | Yes |\n",
    "| **Self-Supervised** | High (GPU) | Medium | Medium (unlabeled) | Partial |\n",
    "| **Diffusion-Based** | Very High | Medium | Large | Yes |\n",
    "\n",
    "### Strengths and Weaknesses\n",
    "\n",
    "#### Classical Methods (Region Growing → Graph-Based)\n",
    "\n",
    "**Strengths:**\n",
    "- ✓ No training data required\n",
    "- ✓ Fast execution on CPU\n",
    "- ✓ Interpretable - clear rules\n",
    "- ✓ Work well for simple, uniform regions\n",
    "- ✓ Low computational requirements\n",
    "\n",
    "**Weaknesses:**\n",
    "- ✗ No semantic understanding (doesn't know \"car\" vs \"tree\")\n",
    "- ✗ Sensitive to parameters\n",
    "- ✗ Struggle with complex textures, lighting variations\n",
    "- ✗ May require manual initialization (seeds, contours)\n",
    "- ✗ Limited to low-level features (intensity, color)\n",
    "\n",
    "#### Deep Learning Methods (CNN+Transformer, Self-Supervised, Diffusion)\n",
    "\n",
    "**Strengths:**\n",
    "- ✓ Semantic understanding (recognizes object classes)\n",
    "- ✓ Superior performance on complex scenes\n",
    "- ✓ Handles lighting, texture, scale variations\n",
    "- ✓ State-of-the-art accuracy on benchmarks\n",
    "- ✓ Can leverage pretrained models (transfer learning)\n",
    "\n",
    "**Weaknesses:**\n",
    "- ✗ Requires large labeled datasets (or unlabeled for SSL)\n",
    "- ✗ Computationally expensive (GPU recommended)\n",
    "- ✗ \"Black box\" - less interpretable\n",
    "- ✗ May not generalize well outside training domain\n",
    "- ✗ Longer inference time\n",
    "\n",
    "### Use Case Recommendations\n",
    "\n",
    "**Choose Classical Methods When:**\n",
    "- Working with simple, uniform regions\n",
    "- Limited computational resources (CPU only)\n",
    "- No labeled training data available\n",
    "- Need explainable results\n",
    "- Real-time performance critical on low-power devices\n",
    "\n",
    "**Choose Deep Learning When:**\n",
    "- Need semantic understanding (object classification)\n",
    "- Working with complex, real-world scenes\n",
    "- Have access to labeled data or pretrained models\n",
    "- GPU resources available\n",
    "- Accuracy is more important than speed\n",
    "\n",
    "### Evolution and Future Directions\n",
    "\n",
    "The field is moving toward:\n",
    "- **Hybrid approaches**: Combining classical reliability with DL accuracy\n",
    "- **Self-supervised learning**: Reducing annotation burden\n",
    "- **Efficient architectures**: Lighter models for edge devices\n",
    "- **Foundation models**: Large pretrained models adaptable to many tasks\n",
    "- **Interactive segmentation**: Human-in-the-loop refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## 6. Conclusion\n",
    "\n",
    "### Project Summary\n",
    "\n",
    "This project explored **9 different approaches to image segmentation**, spanning four decades of computer vision research:\n",
    "\n",
    "**Classical Methods (1980s-2000s):**\n",
    "1. Region Growing - Seed-based expansion\n",
    "2. Watershed - Topographic flooding\n",
    "3. Mean Shift - Mode-seeking clustering\n",
    "4. Split & Merge - Quadtree partitioning\n",
    "5. Active Contours - Energy minimization\n",
    "6. Graph-Based - MST clustering\n",
    "\n",
    "**Modern Deep Learning (2020s):**\n",
    "7. CNN + Transformer Hybrid - SegFormer\n",
    "8. Self-Supervised Learning - Unlabeled data\n",
    "9. Diffusion Models - Iterative refinement\n",
    "\n",
    "Each technique was implemented by a team member in a separate Jupyter notebook, following the DCIT407 project guidelines.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "**1. No Single \"Best\" Method**\n",
    "- Classical methods excel at speed, interpretability, and simplicity\n",
    "- Deep learning methods excel at accuracy and semantic understanding\n",
    "- The right choice depends on: application requirements, available data, computational budget\n",
    "\n",
    "**2. Evolution of Segmentation**\n",
    "- From hand-crafted rules (region growing) to learned representations (deep learning)\n",
    "- From pixel grouping to semantic understanding\n",
    "- From CPU-only to GPU-accelerated processing\n",
    "\n",
    "**3. Practical Implications**\n",
    "- Medical imaging: Classical methods still widely used (U-Net, active contours)\n",
    "- Autonomous driving: Deep learning dominant (SegFormer, transformers)\n",
    "- Resource-constrained devices: Classical methods preferred\n",
    "- Research frontier: Self-supervised and diffusion models\n",
    "\n",
    "### Real-World Impact\n",
    "\n",
    "These techniques power applications we interact with daily:\n",
    "\n",
    "- **Medical Diagnosis**: Tumor detection, organ segmentation (Region Growing, Active Contours)\n",
    "- **Autonomous Vehicles**: Lane detection, object recognition (CNN+Transformer)\n",
    "- **Photo Editing**: Background removal, object selection (Graph-Based, Mean Shift)\n",
    "- **Satellite Imagery**: Land use analysis, urban planning (Watershed, Split-Merge)\n",
    "- **Agriculture**: Crop monitoring, disease detection (Region Growing)\n",
    "- **Manufacturing**: Defect detection, quality control (Classical methods)\n",
    "- **Augmented Reality**: Scene understanding (Deep Learning)\n",
    "\n",
    "### Lessons Learned\n",
    "\n",
    "**Technical:**\n",
    "- Parameter tuning is critical for classical methods\n",
    "- Pretrained models dramatically reduce deep learning complexity\n",
    "- No method works universally - domain matters\n",
    "- Preprocessing (filtering, normalization) significantly impacts results\n",
    "\n",
    "**Collaborative:**\n",
    "- Team division by technique enabled focused expertise\n",
    "- Standardized notebook structure improved consistency\n",
    "- Version control (GitHub) essential for group projects\n",
    "- Documentation quality directly affects usability\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "**For This Project:**\n",
    "- Quantitative comparison: Test all methods on same dataset, compute mIoU/accuracy\n",
    "- Fine-tuning: Adapt pretrained models to specific domain\n",
    "- Ensemble methods: Combine multiple techniques for robust results\n",
    "- Interactive tools: Build UI for parameter adjustment\n",
    "\n",
    "**For the Field:**\n",
    "- **Foundation models**: Large pretrained models (e.g., SAM - Segment Anything Model)\n",
    "- **Efficient architectures**: Real-time transformers for mobile devices\n",
    "- **Multimodal learning**: Combining vision, language, and other modalities\n",
    "- **Few-shot segmentation**: Learning from minimal examples\n",
    "- **Interactive segmentation**: Human-in-the-loop refinement\n",
    "\n",
    "### Acknowledgments\n",
    "\n",
    "This project was completed as part of DCIT407 (Image Processing) at the University of Ghana. We thank:\n",
    "- Our instructor for guidance and resources\n",
    "- Open-source community (OpenCV, PyTorch, Hugging Face) for excellent libraries\n",
    "- Authors of original papers for foundational research\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "Image segmentation is a **fundamental computer vision task** that has evolved dramatically from simple region-based methods to sophisticated deep learning approaches. Understanding both classical and modern techniques provides:\n",
    "\n",
    "- **Historical context**: Appreciation of how the field developed\n",
    "- **Tool diversity**: Ability to choose the right method for each problem\n",
    "- **Foundation for innovation**: Building blocks for future research\n",
    "\n",
    "As the field continues to advance, the principles learned here—similarity, homogeneity, spatial coherence, and semantic understanding—will remain central to how we partition and analyze visual information.\n",
    "\n",
    "---\n",
    "\n",
    "*Project completed by Group 19, DCIT407, February 2026*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "references",
   "metadata": {},
   "source": [
    "<a id=\"references\"></a>\n",
    "## 7. References\n",
    "\n",
    "### Textbooks\n",
    "\n",
    "1. Gonzalez, R. C., & Woods, R. E. (2018). *Digital Image Processing* (4th ed.). Pearson. \n",
    "   - Chapter 10: Image Segmentation\n",
    "\n",
    "2. Szeliski, R. (2022). *Computer Vision: Algorithms and Applications* (2nd ed.). Springer.\n",
    "   - §5.4 Segmentation\n",
    "   - §5.3 Deep Learning for Recognition\n",
    "\n",
    "### Classical Methods - Original Papers\n",
    "\n",
    "3. **Region Growing:**  \n",
    "   Adams, R., & Bischof, L. (1994). Seeded region growing. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 16(6), 641-647.\n",
    "\n",
    "4. **Watershed:**  \n",
    "   Vincent, L., & Soille, P. (1991). Watersheds in digital spaces: An efficient algorithm based on immersion simulations. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 13(6), 583-598.\n",
    "\n",
    "5. **Mean Shift:**  \n",
    "   Comaniciu, D., & Meer, P. (2002). Mean shift: A robust approach toward feature space analysis. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 24(5), 603-619.\n",
    "\n",
    "6. **Split and Merge:**  \n",
    "   Horowitz, S. L., & Pavlidis, T. (1976). Picture segmentation by a tree traversal algorithm. *Journal of the ACM*, 23(2), 368-388.\n",
    "\n",
    "7. **Active Contours:**  \n",
    "   Kass, M., Witkin, A., & Terzopoulos, D. (1988). Snakes: Active contour models. *International Journal of Computer Vision*, 1(4), 321-331.\n",
    "\n",
    "8. **Graph-Based (Felzenszwalb):**  \n",
    "   Felzenszwalb, P. F., & Huttenlocher, D. P. (2004). Efficient graph-based image segmentation. *International Journal of Computer Vision*, 59(2), 167-181.\n",
    "\n",
    "### Modern Deep Learning - Papers\n",
    "\n",
    "9. **U-Net (Foundational CNN):**  \n",
    "   Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional networks for biomedical image segmentation. *MICCAI 2015*. [arXiv:1505.04597](https://arxiv.org/abs/1505.04597)\n",
    "\n",
    "10. **SegFormer (CNN + Transformer Hybrid):**  \n",
    "    Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J. M., & Luo, P. (2021). SegFormer: Simple and efficient design for semantic segmentation with transformers. *NeurIPS 2021*. [arXiv:2105.15203](https://arxiv.org/abs/2105.15203)\n",
    "\n",
    "11. **Vision Transformers:**  \n",
    "    Dosovitskiy, A., Beyer, L., Kolesnikov, A., et al. (2021). An image is worth 16x16 words: Transformers for image recognition at scale. *ICLR 2021*. [arXiv:2010.11929](https://arxiv.org/abs/2010.11929)\n",
    "\n",
    "12. **Attention Mechanism:**  \n",
    "    Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). Attention is all you need. *NeurIPS 2017*. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "13. **Self-Supervised Learning (DINO):**  \n",
    "    Caron, M., Touvron, H., Misra, I., et al. (2021). Emerging properties in self-supervised vision transformers. *ICCV 2021*. [arXiv:2104.14294](https://arxiv.org/abs/2104.14294)\n",
    "\n",
    "14. **Diffusion Models for Segmentation:**  \n",
    "    Baranchuk, D., Rubachev, I., Voynov, A., Khrulkov, V., & Babenko, A. (2022). Label-efficient semantic segmentation with diffusion models. *ICLR 2022*. [arXiv:2112.03126](https://arxiv.org/abs/2112.03126)\n",
    "\n",
    "### Datasets\n",
    "\n",
    "15. **Cityscapes:**  \n",
    "    Cordts, M., Omran, M., Ramos, S., et al. (2016). The Cityscapes dataset for semantic urban scene understanding. *CVPR 2016*.\n",
    "\n",
    "16. **COCO (Common Objects in Context):**  \n",
    "    Lin, T.-Y., Maire, M., Belongie, S., et al. (2014). Microsoft COCO: Common objects in context. *ECCV 2014*.\n",
    "\n",
    "### Software Libraries\n",
    "\n",
    "17. **OpenCV Documentation:**  \n",
    "    [https://docs.opencv.org/](https://docs.opencv.org/)\n",
    "\n",
    "18. **Scikit-Image Documentation:**  \n",
    "    [https://scikit-image.org/](https://scikit-image.org/)\n",
    "\n",
    "19. **PyTorch:**  \n",
    "    [https://pytorch.org/](https://pytorch.org/)\n",
    "\n",
    "20. **Hugging Face Transformers:**  \n",
    "    [https://huggingface.co/docs/transformers](https://huggingface.co/docs/transformers)\n",
    "\n",
    "21. **Segmentation Models PyTorch:**  \n",
    "    [https://github.com/qubvel-org/segmentation_models.pytorch](https://github.com/qubvel-org/segmentation_models.pytorch)\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "22. **Region-Based Segmentation Survey:**  \n",
    "    Pal, N. R., & Pal, S. K. (1993). A review on image segmentation techniques. *Pattern Recognition*, 26(9), 1277-1294.\n",
    "\n",
    "23. **Deep Learning for Segmentation Survey:**  \n",
    "    Minaee, S., Boykov, Y., Porikli, F., Plaza, A., Kehtarnavaz, N., & Terzopoulos, D. (2021). Image segmentation using deep learning: A survey. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 44(7), 3523-3542.\n",
    "\n",
    "---\n",
    "\n",
    "*End of Master Notebook - Group 19, DCIT407 Image Processing Project*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
